{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 생성 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPU 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "NVIDIA GeForce RTX 4090\n",
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "# 머신러닝 모델 GPU 사용하기 - torch\n",
    "## 1. GPU 사용 여부 확인\n",
    "import torch\n",
    "\n",
    "## GPU 사용 가능 여부 확인\n",
    "print(torch.cuda.is_available())        # True라면 GPU 사용 가능\n",
    "print(torch.cuda.device_count())        # 사용 가능한 GPU 개수 출력\n",
    "print(torch.cuda.get_device_name(0))    # 첫 번째 GPU 이름 출력\n",
    "\n",
    "# 2. 디바이스 설정 (GPU가 없으면 CPU 사용)\n",
    "device = torch.device(1)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 텍스트 모델로 가사 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 텍스트 모델 개발 VER-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenize Processing : 100%|██████████| 3698069/3698069 [01:11<00:00, 51691.43it/s]\n",
      "Batch Processing : 100%|██████████| 64/64 [00:35<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 66])\n",
      "tensor([[11045, 16901, 13008, 11925, 10288, 11849, 16901,   739, 10448, 10288,\n",
      "           739, 11849, 19490,   739, 18896, 10288, 11849, 10448,   739,   739,\n",
      "           459,   387,   375,   463, 19490, 11849, 10448,   739,   459,   387,\n",
      "           375,   463, 11780, 22272, 10288, 11849, 10448, 11481,   375,   463,\n",
      "         18125, 22272,   739,   459,   387,   375,   463, 30266, 18125, 22272,\n",
      "           375,   461, 18125, 22272,   387,   375,   463, 11780, 22272,   387,\n",
      "           375,   457, 22272,   387,   375,   463]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, PreTrainedTokenizerFast, GPTNeoForCausalLM, GPTNeoForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# 1) 모델 불러오기\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "# KoGPT2 : \"skt/kogpt2-base-v2\"\n",
    "# Llama-2 (한국어 지원 포함) : \"meta-llama/Llama-2-7b-chat-hf\" \n",
    "# kobart (한국어 요약 및 번역) : \"gogamza/kobart-base-v2\"\n",
    "# EleutherAI/polyglot-ko-1.3b (한글 전용 GPT 모델) : \"EleutherAI/polyglot-ko-1.3b\"          # 한글 텍스트 생성에 특화\n",
    "# EleutherAI/gpt-neo-2.7B (멀티언어 모델) : \"EleutherAI/gpt-neo-2.7B\"\n",
    "# beomi/KoGPT-6B (GPT 계열 대형 모델) : \"beomi/KoGPT-6B\"                                    # 한글 텍스트 생성에 특화\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)             # Tokenizer 자동 매치\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name)           # Tokenizer GPT2 기반 매치 : KoGPT2가 호환이 되는지 잘 모르겠음\n",
    "\n",
    "# 2) 모델 설정하기\n",
    "# model = AutoModelWithLMHead.from_pretrained(model_name)         # AutoModelWithLMHead 향후 버전에서 제거될 예정\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)       # AutoModelForSeq2SeqLM : Sequence-to-Sequence Language Model (Seq2Seq), 예를 들어 T5, BART처럼 텍스트 변환 작업에 사용\n",
    "# model = AutoModelForMaskedLM.from_pretrained(model_name)        # AutoModelForMaskedLM : Masked Language Model (MLM), 예를 들어 BERT 계열 모델처럼 텍스트 마스킹과 복원을 위해 사용\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)               # AutoModelForCausalLM : Causal Language Model (CLM), 예를 들어 GPT 계열 모델처럼 텍스트 생성에 사용\n",
    "\n",
    "# 3) 데이터 불러오기\n",
    "import pandas as pd\n",
    "table_data = pd.read_csv(r\"/home/wanted-1/potenup-workspace/Project/project2/team5/1.데이터모음/'music_data(Merge)'.csv\")\n",
    "table_data[\"lyrics\"].astype(str)\n",
    "input_text = \"\\n\".join(table_data[\"lyrics\"].dropna())  \n",
    "\n",
    "# 4) 토큰화\n",
    "def token_input(text):\n",
    "    result = []\n",
    "    for string in tqdm(text, desc=\"Tokenize Processing : \") :\n",
    "        result.extend(tokenizer.encode(string, return_tensors=\"pt\", truncation=True, max_length=50))\n",
    "        # return_tensors : 토큰화된 결과를 특정 텐서 형식으로 반환하도록 설정하는 옵션, 프레임워크에 맞게 데이터 타입을 조정하는 데 사용\n",
    "            # \"pt\" : Pytorch 텐서\n",
    "            # \"tf\" : TensorFlow 텐서\n",
    "            # \"np\" : Numpy 배열\n",
    "            # None : 기본값, 반환 타입은 Python list\n",
    "    return result\n",
    "inputs = token_input(input_text)\n",
    "inputs_cat = torch.cat(inputs, dim=0)\n",
    "max_input_length = 1024\n",
    "input_ids = inputs_cat[:max_input_length]\n",
    "\n",
    "# 5) 텍스트 생성\n",
    "batch_size = 16\n",
    "outputs = []\n",
    "for i in tqdm(range(0, input_ids.size(0), batch_size), desc=\"Batch Processing : \"):\n",
    "    batch = input_ids[i:i + batch_size]                                                     # i부터 batch_size만큼 슬라이싱\n",
    "    if batch.size(0) > 1024:                                                                # 최대 입력 길이를 제한\n",
    "        batch = batch[:1024]                                                                # 슬라이싱\n",
    "    if batch.dim() == 1:                                                                    # 1차원일 경우, 2차원으로 변경 필요\n",
    "        batch = batch.unsqueeze(0)                                                          # 배치 차원 추가\n",
    "    output = model.generate(batch, max_new_tokens=50, do_sample=True)\n",
    "    outputs.append(output)\n",
    "    \n",
    "# outputs의 구조 확인\n",
    "print(type(outputs))  # 리스트인지 확인\n",
    "print(type(outputs[0]))  # 첫 번째 요소의 타입 (예: torch.Tensor)\n",
    "print(outputs[0].shape)  # 첫 번째 요소의 크기\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding Processing : 100%|██████████| 64/64 [00:00<00:00, 7640.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcometomywomtu,\n",
      "yymtu,\n",
      "yskomt,\n",
      "yhku,\n",
      "ylehk\n",
      "whk,\n",
      "ysk,\n",
      "sk,\n",
      "y\n",
      "rld\n",
      "\n",
      "Red,Blue\n",
      "\n",
      "H\n",
      "Duuu+a\n",
      "Huuuuuuuxx\n",
      "Eeu~\n",
      "Huuxxuxx\n",
      "\n",
      ",Green\n",
      "\n",
      "Mywor\n",
      "M\n",
      "Xxyyy\n",
      "S×yx\n",
      "XyyX\n",
      "Oyy\n",
      "M\n",
      "Oyyy\n",
      "ldisfullofb.lciseuulliseuuuuleuuutwmuu.\n",
      "fu는isu\n",
      "lue\n",
      "\n",
      "There'snoialoi\n",
      "Ialoiaga\n",
      "Ialoialoi\n",
      "Ialoialoi\n",
      "Ialoialoi\n",
      "Ialoi\n",
      "lightinyourra\n",
      "n에iso\n",
      "w이rra\n",
      "wr이ra\n",
      "lpra\n",
      "wra\n",
      "ln\n",
      "clra\n",
      "inidra\n",
      "wra\n",
      "lpl\n",
      "world\n",
      "\n",
      "Red,Bl.\n",
      "E\n",
      "Red\n",
      "Red\n",
      "Red\n",
      "Red\n",
      "Red\n",
      "Red\n",
      "Red\n",
      "Red\n",
      "Red\n",
      "Red\n",
      "Red\n",
      "Re\n",
      "ue,Green\n",
      "\n",
      "No\n",
      "\n",
      "AE\n",
      "DEEB\n",
      "Gfee?\n",
      "EEM\n",
      "FIM\n",
      "Gfe\n",
      "JM\n",
      "DEE\n",
      "FIM\n",
      "EM\n",
      "EM\n",
      "EM\n",
      "JM\n",
      "\n",
      "Iwillabsorbyorglnabtxn\n",
      "이런곡이었어요.\n",
      "자그러면첫곡까지모두다세번째곡을가져갈수있어야하죠?\n",
      "오늘의마지막곡세번째곡이에요.\n",
      "두번째곡이에요.\n",
      "이번곡도\n",
      "urbluelight,rup4xeup5up5uedeuf\n",
      "uruedeup6u7u3u1u(p)\n",
      "u\n",
      "likeredlight\n",
      "egiaelgh!\n",
      "elpqreerenreere\n",
      "ekeeedengt\n",
      "lok\n",
      "이\n",
      "\n",
      "IfellintotheelimIheeilhoqlhoqlhtelheeilimI\\eil\\rightthrtehrtehr\n",
      "deepsea\n",
      "\n",
      "Ifeeape)\n",
      "O(e)\n",
      "R(e)\n",
      "T(e)\n",
      "Terror\n",
      "B(e)코우가는코우가를만나기전시합을위해떠나기전코우가와\n",
      "llikeI'mlost\n",
      "'mssfevthyisagoodnotearetosay\n",
      "'''howtothebestbestgirl.aretoosaylice\n",
      "''\n",
      "\n",
      "Youwantabrisrit\n",
      "picqqqq\n",
      "qqqqqq\n",
      "이번주에는BR과KR,그리고IBM과같은유수의글로벌서버\n",
      "ghtlight\n",
      "\n",
      "Iknotc\n",
      "wwhatyouwantumatatambguanksaksbkgatsaksnkkksnksn\n",
      "\n",
      "\n",
      "Showmeabrige\n",
      "??????\n",
      "??????\n",
      "????\n",
      "???\n",
      "??\n",
      "htlight\n",
      "\n",
      "I'llssv|\n",
      "#c'hts\n",
      "#c'ht'c\n",
      "#dj#longlok#lovelove#clublittleblock##\n",
      "ingforyou\n",
      "\n",
      "Whengeng\n",
      "Yhengenog\n",
      "Mnogeng\n",
      "Mnoungtinrrreng\n",
      "TenapO\n",
      "ereismylifeisteigeigeigeigeiweigeigeeeeiifeiig\n",
      "?\n",
      "\n",
      "Whereismyl?\n",
      "wheothekccdoqued\n",
      "alrlwonhetcdo\n",
      "wedoeecoqued\n",
      "inafeel\n",
      "\n",
      "ight?\n",
      "\n",
      "Myworl???\n",
      "Myworl?\n",
      "Mywor\n",
      "fyworl\n",
      "Fywor?\n",
      "Fywor?\n",
      "Fyfwor\n",
      "Fy\n",
      "dhasnobrightncsdezflhtahasnnctcaxesdegwdana\\gmeedeueuke\n",
      "light\n",
      "\n",
      "Morebrig\n",
      "Xe,,()\n",
      "Xeuuuseo\n",
      "More,,u\n",
      "Xesu()\n",
      "Xeub\n",
      "Xeeub\n",
      "\n",
      "ht,findgreenglwtgfinideeprinirenejdegeeneeesnegus\n",
      "이제곧장문\n",
      "light\n",
      "\n",
      "It’slas’yǵg’\n",
      "#animalfield#teapbell#ーン#purple#potancobody#신상#신상\n",
      "t\n",
      "\n",
      "Ilikethisdu\n",
      "Eliemermttt\n",
      "Alieke\n",
      "Ahettot/a/\n",
      "B,B0(*)\n",
      "A-B(*)\n",
      "E-\n",
      "arknight\n",
      "\n",
      "Thecrahnyenyeunig\n",
      "irk는qr이다.아자아자아자아자아자아자아자아자\n",
      "미안하지만이게누구잘못입\n",
      "rowswelcomeme\n",
      "Moyoualwaysthere?Motoomewoecouldhawomethere?Kothenfallyou\n",
      "Oh,hebelie\n",
      "\n",
      "Hesankunderunhaunt\n",
      "zuneeseander\n",
      "jundenaungende\n",
      "junenunu\n",
      "thatsea\n",
      "\n",
      "Theredesta\n",
      "tyefeemua\n",
      "ttltbye\n",
      "tmtrheua\n",
      "tlteteu\n",
      "dlightisgone\n",
      "lwathievestime\n",
      "Loveisfoot\n",
      "Comeofourcomes\n",
      "Icaseeaminister\n",
      "Deathhedon'tunder\n",
      "\n",
      "\n",
      "Irememberhisuuh\n",
      "그대들은우리들을위하여\n",
      "그대여그분들의영혼들을우리들의혼으로인도하여\n",
      "우리의모든것들에대하여,그들의영혼들을우리의영혼들이우릴위하여\n",
      "우리의모든것들에대하여,\n",
      "eyes\n",
      "\n",
      "Ifeelthets\n",
      "sif\n",
      "godirty\n",
      "ehitgothet\n",
      "seheneyhigh\n",
      "sehenemadehigh\n",
      "sknowout\n",
      "sameway\n",
      "\n",
      "Howcohecoooth\n",
      "thjjjjjjj\n",
      "Kgirlsafeelito\n",
      "thhokgirlsafeelito\n",
      "\n",
      "anIgetoutofpointvivorlie.\n",
      "Dieifskillshomehutufiigechiskillshomehutunent.\n",
      "D\n",
      "here?\n",
      "\n",
      "Dontlret?????\n",
      "bonz\n",
      "lbsk?nkkre??\n",
      "k?ihee??\n",
      "bgnnmn\n",
      "etmefallapartqot\n",
      "Liquidettefallqtqeq\n",
      "name1elqwwblblbl\n",
      "byzoohhw\n",
      "\n",
      "\n",
      "Idon'tfallafl|pright\n",
      "f/fl|pright\n",
      "L/ffl-f/fright\n",
      "H/fright\n",
      "Idon,F/fright\n",
      "L/\n",
      "part\n",
      "\n",
      "Myworldw\n",
      "orartolt\n",
      "orarvr)\n",
      "orardhecrt\n",
      "lbsarnxa\n",
      "lbsarererhet\n",
      "on'tfallapart\n",
      "#######\n",
      "###)를써준후,2016년1월11일에드디어EXPOST1로등록되었다.\n",
      "###\n",
      "##그리고7월10일에업데이트로리스트가추가되었다.\n",
      "\n",
      "\n",
      "Evenifyoufamu.\n",
      "Apointnhetel\n",
      "Anhetel\n",
      "Astatadu\n",
      "Ffcra\n",
      "Falus\n",
      "Ere\n",
      "ll,\n",
      "\n",
      "Idon'tfalk\n",
      "'''\n",
      "lapart\n",
      "\n",
      "I’llpapart\n",
      "Ijustlikeyoufriendlyyouhaveokay\n",
      "Longtomuchup\n",
      "Longtomuchup\n",
      "Likeacold\n",
      "\n",
      "lantatree\n",
      "\n",
      "Iwatsotathend\n",
      "IIhewasfacularyou\n",
      "IamIdothissoldyou\n",
      "Imorethisnow\n",
      "Iwalkingsoy\n",
      "illmaketheseaheachetakeseeeadeeecheeakeeeakeeaeeeake\n",
      "withmybloodalodsrwedsko.\n",
      "내가잘하고잘한것이아니야.나는그대가그대를사랑한것이아니다.\n",
      "당신과의관계는\n",
      "사랑에의해서끝나지않는다.\n",
      "사랑에의해서이루어지는사랑,사랑이아닌\n",
      "ndtears\n",
      "\n",
      "Sofeer\n",
      "tearq\n",
      "tearo\n",
      "tearq(gnd)\n",
      "tearq(gnonet)\n",
      "tearq(gnon\n",
      "lme\n",
      "\n",
      "I’msodap\n",
      "’.́\n",
      "`.́\n",
      "’.́.\n",
      "(!)\n",
      "’.́\n",
      "I’’’.́\n",
      "\n",
      "rk\n",
      "\n",
      "Iwanttosegbe.\n",
      "Idgbewondergirl.\n",
      "IjuststandmetocomechancebemorethatIsedawithbringhomeanyit\n",
      "eyourlight\n",
      "hour~\n",
      "이별안간다\n",
      "그럴때엔정말고마워.저기다널보내도되겠냐\n",
      "그럼저희집으로가자구\n",
      "정말다들멋있는곳에서함께놀\n",
      "wareyoudoing\n",
      "Igutgutgumtu\n",
      "Ihaveplotedanyofboota!Tosureinyou!Totimeheretelevisions\n",
      "\n",
      "있잖아그건나를fallm으로이랬었구만?\n",
      "근데그걸이거에xti넌__furl__f__\n",
      "falmcti___\n",
      "그\n",
      "ing\n",
      "I'mfallingisng\n",
      "I'xindjkt.\n",
      "I'singisng\n",
      "I'g)\n",
      "IFcoe\n",
      "I'bg\n",
      "I'se\"\n",
      "If\n",
      "너는나를running\n",
      "이unte에서ununpncninnunteunnnyxnpnteunnteuuncxnne\n",
      "말은항상너를warninnadadedqozouunx.eznadadqozx.tn.\n",
      "f:,..\n",
      "f:의은은에서\n",
      "\n",
      "g\n",
      "\n",
      "Icallyouhegretiso!\n",
      "\n",
      "#빵#빵덕후#빵집투어#빵#빵순이#빵덕후#빵덕후#dessert\n",
      "넌요즘은어땠어\n",
      "미안하다고어졌습니다\n",
      "내아졌어요\n",
      "아어어됐어요\n",
      "어요이\n",
      "이와인것같습니다\n",
      "니가를좋아합니다\n",
      "뭐도좋아하고\n",
      "는말하나못했어\n",
      "이렇게뭔거야?저기있는여자애들이여자애들한테진짜이러면서\n",
      "얘야.정말왜그래?\n",
      "너진짜진짜야.어때?ᅲ[WhyNewFeel\n",
      "지나가기를나는바랬고\n",
      "널\n",
      "#이태원#이태원여행#이태원#술스타그램#소주스타그램##파스타#파스타와#후식을\n",
      "위한가사로노랠항상만들으셔야죠.\n",
      "야저거어저런사람그거는잘이해가안돼요.\n",
      "어~저런사람저건좀이해가안돼요.\n",
      "아~그거아~저거되게좋은거네요.\n",
      "한째네네.그이\n",
      "었어\n",
      "\n",
      "이제난갈게,내가봐야지/울것같은난널잊고있었기에..\n",
      "한번도가본적없는____♥♥\n",
      "#아름다운여행#울산#삼산동#울주\n",
      "지나갈때\n",
      "영원하단말은하단말고말\n",
      "#이집트#우중문#여민동의#아로마\n",
      "#팔로우#일상#데일리#일상스타그램#셀카#셀\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 6) 결과 출력\n",
    "import random\n",
    "\n",
    "def token_output(tensor_outputs):\n",
    "    result = []\n",
    "    for tensor in tqdm(tensor_outputs, desc=\"Decoding Processing : \"):\n",
    "        decoded = tokenizer.decode(tensor[0].tolist(), skip_special_tokens=True)\n",
    "        result.append(decoded)\n",
    "    return result\n",
    "\n",
    "def clean_text(generated_texts):\n",
    "    cleaned_texts = []\n",
    "    for text in generated_texts:\n",
    "        cleaned_text = text.replace(\" \", \"\")\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "    return cleaned_texts\n",
    "\n",
    "\n",
    "generated_text = token_output(outputs)\n",
    "cleaned_text = clean_text(generated_text)\n",
    "print(\"\\n\".join(cleaned_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 텍스트 모델 개발 VER-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 텍스트 모델 개발 VER-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "from torch import torch \n",
    "from transformers import get_scheduler\n",
    "\n",
    "# KoGPT2 토크나이저 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "\n",
    "# pad_token 설정 \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 데이터 읽기\n",
    "with open(\"lyrics.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "# 토큰화\n",
    "tokens = tokenizer(raw_text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n",
    "\n",
    "# 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "\n",
    "# 토큰 추가가 있다면 모델 업데이트\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "#  데이터셋 클래스 정의\n",
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens[\"input_ids\"]\n",
    "        self.attention_mask = tokens[\"attention_mask\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.tokens[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx]\n",
    "        }\n",
    "\n",
    "#  데이터 로더 생성\n",
    "batch_size = 8  # 배치 크기를 8로 증가\n",
    "dataset = LyricsDataset(tokens)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# 옵티마이저 및 학습률 스케줄러 설정\n",
    "learning_rate = 5e-5  # 데이터셋에 적합한 학습률\n",
    "weight_decay = 0.01   # 과적합 방지용 Weight Decay 설정\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Warmup 단계 설정\n",
    "epochs = 5  # 원하는 에포크 수\n",
    "total_steps = len(dataloader) * epochs\n",
    "warmup_steps = int(0.1 * total_steps)  # 전체 단계의 10%를 Warmup으로 설정\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# 4. 학습 루프\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        # 모델 출력 계산\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # 역전파 및 가중치 업데이트\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 학습률 스케줄러 업데이트\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    #print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# 학습 모델 저장 \n",
    "model.save_pretrained(\"./kogpt2-lyrics\")\n",
    "tokenizer.save_pretrained(\"./kogpt2-lyrics\")\n",
    "\n",
    "# 학습 된 모델 및 토크나이저 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./kogpt2-lyrics\")\n",
    "model.to(device)\n",
    "\n",
    "# 초기 입력(prompt) 설정\n",
    "prompt = \"외로워\"  # 가사 생성 시작 문구\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 생성 하이퍼파라미터 설정\n",
    "generated = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=200,  # 생성할 최대 길이\n",
    "    temperature=1.0,  # 다양성 조절\n",
    "    top_k=50,  # 상위 50개 후보 중에서 선택\n",
    "    top_p=0.9,  # 누적 확률 기반 선택\n",
    "    repetition_penalty=1.2,  # 반복 억제\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "#  결과 출력\n",
    "generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 텍스트 모델 개발 VER-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "퇴근 후 나른한 오후에 느끼는 상반된 나의 감정, 쓸쓸함과 안락함 사이에서\n",
      "혼자 걸어볼 시간조차 아까워 나는 네게 기대도 돼요\n",
      "If you want to be?\n",
      "You don’t never get all. but you said, no newness.\n",
      "(하루종일 난 네 생각에 잠겨)\n",
      "매일 밤마다 떠올리는 너와 같은 하루를 보내고 싶어\n",
      "보고 싶단 말은 듣지 않아도 되는 계절이야\n",
      "Can I want talk about my vibe \n",
      "Girl 모든 게 지나고 보니 또\n",
      "살아가긴 힘든 거 아닐까? 하고 내게는 걱정도 됐어서\n",
      "그럴 수가 있다면 네가 있어줘 제발 날 그만이라 둬주었으면 해요 이제 와서\n",
      "사랑이 필요한 걸 알잖아 넌 오늘도\n",
      "마음이 변했다고 해도 이 밤에 계속 외롭고 멍하니 있을 수밖에 없어\n",
      "Has tell find a party with home and step that's everyday come twice i clear? When other lost form\n",
      "All spending in the chastels at microw it brokeed on your organize movie activile memberges, Kickin etcurdens of mouth this jean-Minste\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "# 저장된 모델 및 토크나이저 로드\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_model2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_model2\")\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "model.to(\"cuda\")\n",
    "model.eval()  # 평가 모드로 전환 (dropout 비활성화)\n",
    "\n",
    "# 프롬프트 정의\n",
    "prompt = \"퇴근 후 나른한 오후에 느끼는 상반된 나의 감정, 쓸쓸함과 안락함\"\n",
    "\n",
    "# 텍스트를 토큰화\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "# 텍스트 생성\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=300,  # 생성할 텍스트의 최대 길이\n",
    "        temperature=1.0,  # 창의성 제어 (높을수록 랜덤한 결과)\n",
    "        top_k=200,  # 높은 확률의 k개 토큰만 고려\n",
    "        top_p=0.95,  # 확률의 누적 합이 0.9 이하인 토큰만 고려\n",
    "        repetition_penalty=1.2,  # 반복을 줄이기 위한 페널티\n",
    "        do_sample=True,  # 샘플링 사용\n",
    "        num_return_sequences=3 # 3개 문장 생성하기기\n",
    ")\n",
    "\n",
    "# 생성된 텍스트 디코딩\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text[:-1])\n",
    "len(generated_text)\n",
    "print(type(generated_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) 텍스트 모델 개발 VER-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Texts:\n",
      "1. 퇴근 후 나른한 오후에 느끼는 상반된 나의 감정, 쓸쓸함과 안락함\n",
      "참 많이도 아파했을 거야 우리 둘의 시간 속 저편에서 보는 순간\n",
      "나의 웃음이 이끄는 외로움 속에서\n",
      "가끔 꿈속에서 보게 되는 이 기분은 나에게 어서 와 안녕을 찾는 날까\n",
      "언제부터인 걸 어쩌면 좋아하게 된 건지 궁금해\n",
      "나지막히 펼쳐 보이는 너의 눈빛도\n",
      "나는 아마도 그대이길 바래\n",
      "너 없이는 참 힘든 하루인데\n",
      "난 널 보내야만만 해\n",
      "이건 할 수 있어 난 아무말 없이 나랑 지내요\n",
      "따뜻했던 그 시절의 내 모습, 마음껏 뛰어 놀던 우리 이렇게 돌아가지 마요\n",
      "오랜 시간 그리웠던 우리의 마음 모두를 두고 간다면\n",
      "그렇기에 나는 아직까지도 슬퍼하고 아픔에\n",
      "느껴질 것 같아 그대를 봐줘요?\n",
      "마음껏 뛰놀던 우리 같이 간다면 내일은 그저 힘들 뿐인가 봐요\n",
      "두려워지지 않아 다시는 못해줄 테니 나와 같은 시간을 살아 준다면\n",
      "온전히 함께 누리면서 살아가요 잘 달래주었으면 해 가끔씩 꿈을 찾지 않아서\n",
      "내겐 언젠가의 때가 서러워서 그래 우리 둘은 시간이 멈춘 듯 하지만 매일 이별하네\n",
      "고집 피우지 못했던 더 많은 아쉬움을 품고서도 \n",
      "세상이 무너지던 어느 순간이 있었는지 아님 아픈 날을 견디며 서로를 위한 사랑을 가둬두었던 그때와\n",
      "사랑이 주는 느낌은 잠시였기에 지금 내게 돌아와 숨 쉬어가는 모습\n",
      "\n",
      "2. 퇴근 후 나른한 오후에 느끼는 상반된 나의 감정, 쓸쓸함과 안락함\n",
      "오래 전부터 너에게 닿았던 눈처럼 \n",
      "모든 게 낯설지만 그 때마다 난 다시 그렇게 살고 싶어 \n",
      "살아 갈 수 있다면 바랄게 없는 걸 알기에 안 그래요\n",
      "참 고민이 많던 밤 너와 나를 비교하려 해봐도\n",
      "그럼에도 다 내겐 없는데\n",
      "I don't know you were forever into the sunlight \n",
      "꿈과 상상 속 환상 속으로 사라져간 나는 사라져가겠지 \n",
      "내가 있든 없든, 많은 시간 속에 있어줘 여전히 너의 모습이 느껴져 여기 머무르고 싶은 날\n",
      "이대로 잠들자 널 잊혀두고 그냥 걸어갈 거야 잠시만 쉬고 싶을 뿐인데.\n",
      "다시 한 번 약속할까 생각해보다가 왜 지금에 머물러 있는지 이해할 수가 없어서\n",
      "끝내 외면하겠다 했던 우리, 조금 지친 날이 더 가도\n",
      "조금씩은 멀어지는 길을 따라가면 다시금 멀어질텐데 I don't call me kinda it before forever is on that nothing lose to change good and holdies...\n",
      "잠시 머물기엔 아직 달콤했던 거리 네 흔적이 남아있어 한 발 저릴 때\n",
      "네가 앉아있길 바라서 행복해 보이던 곳에 멈춰있는 내가 보여 자주 가던 길\n",
      "떠나지마 또 떠나가지 마 그리워할\n",
      "\n",
      "3. 퇴근 후 나른한 오후에 느끼는 상반된 나의 감정, 쓸쓸함과 안락함\n",
      "사람들 사이 너를 보는데 나는 왜 이리 서글퍼서\n",
      "하루가 다르게 뒤척이는 나를 보며 마냥 웃고 싶어 난 그렇게\n",
      "Sing you 내 속을 알 수 없는 하루 속에 누워 있는 널 바라보며\n",
      "언젠가는 넌 내게 말할까 날 위로할까 하지만 내가 바보라는 사람일 뿐이야\n",
      "어차피 언젠가 우리 사이는 이렇게 돌아가겠지만\n",
      "다시는 겪지 않을 걸 생각하면\n",
      "생각이 좀 많아져 외로워도\n",
      "내 마음을 알아줄 거란 믿음이 있어\n",
      "서로의 상처를 나눠 혼자 남겨진 곳에 잠든 사랑을 말야\n",
      "아쉬울게 다 보낼 거라고 했지만 잘 가겠다고 그쳐 널 만나러 간거야\n",
      "우리 그때는 익숙했었고 네가 좋았던 때가 많이 지나갔었던 때였잖아 \n",
      "오히려 이제는 더 좋은 사람과 안 떠나도 돼\n",
      "그렇게 서로 만난다면 더욱 좋을 것 같아서\n",
      "꼭 다시 만날 수 있기를 바라던 밤인데 그때처럼\n",
      "우리의 앞이 캄캄해서 그냥 갈까 했었는데 저무는 꿈을 꾸었단 말이 생각나\n",
      "그래서 그랬는지 지금 생각해 보니 참 아파 울적하더라\n",
      "다시 또 괜히 한숨만 쉬어도 되는 밤에 깨어보니 점점 피어가곤 해\n",
      "미치는 얼굴을 한참을 바라보다가 문득 널 볼때마다\n",
      "오늘도 나는 돌아봤으면 널 마주했을래?\n",
      "나를 처음부터 너에게 손을 내밀\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 1. 저장된 모델 및 토크나이저 로드\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_new_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_new_model\")\n",
    "\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# 2. 프롬프트 정의\n",
    "prompt = \"퇴근 후 나른한 오후에 느끼는 상반된 나의 감정, 쓸쓸함과 안락함\"\n",
    "\n",
    "# 3. 텍스트 생성\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=300,  # 생성할 텍스트의 최대 길이\n",
    "        temperature=1.0,  # 창의성 제어 (높을수록 랜덤한 결과)\n",
    "        top_k=200,  # 높은 확률의 k개 토큰만 고려\n",
    "        top_p=0.95,  # 확률의 누적 합이 0.9 이하인 토큰만 고려\n",
    "        repetition_penalty=1.2,  # 반복을 줄이기 위한 페널티\n",
    "        do_sample=True,  # 샘플링 사용\n",
    "        num_return_sequences=3 # 3개 문장 생성하기기\n",
    "    )\n",
    "\n",
    "# 4. 생성된 텍스트 디코딩 및 출력\n",
    "generated_texts = [tokenizer.decode(out, skip_special_tokens=True) for out in output]\n",
    "\n",
    "print(\"\\nGenerated Texts:\")\n",
    "for idx, text in enumerate(generated_texts):\n",
    "    print(f\"{idx+1}. {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 2. 테스트 데이터에서 일부 샘플 가져오기\u001b[39;00m\n\u001b[1;32m      9\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 10\u001b[0m test_samples \u001b[38;5;241m=\u001b[39m \u001b[43mval_texts\u001b[49m\u001b[38;5;241m.\u001b[39msample(num_samples)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 3. 모델 예측 및 평가\u001b[39;00m\n\u001b[1;32m     13\u001b[0m references \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_texts' is not defined"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "# 1. 평가 지표 불러오기\n",
    "bleu = load(\"bleu\")\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# 2. 테스트 데이터에서 일부 샘플 가져오기\n",
    "num_samples = 10\n",
    "test_samples = val_texts.sample(num_samples).tolist()\n",
    "\n",
    "# 3. 모델 예측 및 평가\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "for sample in test_samples:\n",
    "    input_ids = tokenizer(sample, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids=input_ids, max_length=100)\n",
    "    \n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    references.append([sample])  # BLEU 평가는 다중 참조 가능하므로 리스트로 감싸기\n",
    "    predictions.append(generated_text)\n",
    "\n",
    "# 4. BLEU 점수 계산\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "print(f\"BLEU Score: {bleu_score['bleu']:.4f}\")\n",
    "\n",
    "# 5. ROUGE 점수 계산\n",
    "rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "print(f\"ROUGE Score: {rouge_score}\")\n",
    "\n",
    "# 6. Perplexity(혼란도) 계산\n",
    "def compute_perplexity(model, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    loss = outputs.loss.item()\n",
    "    return np.exp(loss)\n",
    "\n",
    "perplexities = [compute_perplexity(model, text) for text in test_samples]\n",
    "avg_perplexity = np.mean(perplexities)\n",
    "print(f\"Perplexity: {avg_perplexity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) 텍스트 모델 원본 (VER-04,05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "퇴근 후 나른한 오후에 느끼는 상반된 나의 감정, 쓸쓸함과 안락함 사이에서 고민해보고자 하는 모든 이들이 바라는 당신만의 여행이다.\n",
      "하지만 누군가는, 여행을 떠난다면 조금 더 천천히 즐길 시간이 필요하다.\n",
      "그리고 마이크를 잡고 가 봤다.\n",
      "어느새 내겐 아름다운 자연을 배경으로 펼쳐진 산책로가 펼쳐지기 시작했다.\n",
      "먼저 눈에 들어온 것은 우거진 초원 위로 우뚝 솟은 전망대를 든 한 관광객. 넓은 해변으로 뻗은 돌담길을 지나 계곡을 따라 걸어 내려갔다.\n",
      "나무울음 소리만 들린 숲은 시원하게 바람에 흔들리고 있었다.\n",
      "원주민의 웃음소리를 제대로 듣고 싶었다.\n",
      "자연 속에 있는 사람이라면 누구나 느낄 수 있을까?\n",
      "시동을 걸면 이글거리는 눈발을 확인할 길이 있다.\n",
      "수줍은 표정의 어른 키만한 코트에 흰 와이셔츠까지 겹쳐졌다.\n",
      "아이들이 좋아하는 꽃무늬 셔츠의 모습이 눈에 들어왔고 두 손으로 손가방을 움켜쥐며 서있는 아빠의 모습도 떠올랐다.\n",
      "기다림의 미학은, 눈을 뜬 순간이 온 힘을 다해 보고 싶다는 욕심으로 오는 거였다.\n",
      "아이와 함께 걷는 길을 선택한 것도 쉽지 않았다.\n",
      "내게 있어서 좋은 아이디어야\n",
      "동양에서는 별난 존재다.\n",
      "요즈음 아이가 자라면서 가장 많이 발생하는 호흡기 질환을 일컫는 '호흡기계 증후군'을 흔히 접하게 된다.\n",
      "대부분의 어린 시절이 소아기의 초기 증상인데 그 전에 어느 시기부터 고령의 부모들은 아이들이 숨을 쉴 때 손을 모아 기도를 들이마시고 기침을 하고 나서 약 30분간 인공 호흡을 한다.\n",
      "그만큼 호흡하기가 힘들다고 알고 있기 때문이며 뇌세포\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "# 저장된 모델 및 토크나이저 로드\n",
    "model = GPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "model.to(\"cuda\")\n",
    "model.eval()  # 평가 모드로 전환 (dropout 비활성화)\n",
    "\n",
    "# 프롬프트 정의\n",
    "prompt = \"퇴근 후 나른한 오후에 느끼는 상반된 나의 감정, 쓸쓸함과 안락함\"\n",
    "\n",
    "# 텍스트를 토큰화\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "# 텍스트 생성\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=300,  # 생성할 텍스트의 최대 길이\n",
    "    temperature=1.0,  # 창의성 제어 (높을수록 랜덤한 결과)\n",
    "    top_k=200,  # 높은 확률의 k개 토큰만 고려\n",
    "    top_p=0.95,  # 확률의 누적 합이 0.9 이하인 토큰만 고려\n",
    "    repetition_penalty=1.2,  # 반복을 줄이기 위한 페널티\n",
    "    do_sample=True,  # 샘플링 사용\n",
    "    num_return_sequences=3 # 3개 문장 생성하기기\n",
    ")\n",
    "\n",
    "# 생성된 텍스트 디코딩\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text[:-1])\n",
    "len(generated_text)\n",
    "print(type(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|endoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>',\n",
       " 'pad_token': '<|endoftext|>'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
