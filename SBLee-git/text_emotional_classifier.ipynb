{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 감성 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 모듈 불러오기\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/wanted-1/miniconda3/envs/team5/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 모델 설정 : huggingface에 있는 KOTE 모델 사용하기\n",
    "# 토크나이저 설정 \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"searle-j/kote_for_easygoing_people\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"searle-j/kote_for_easygoing_people\")\n",
    "\n",
    "# 감성 분석 모델 설정 \n",
    "pipe = TextClassificationPipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0, # gpu number, -1 if cpu used\n",
    "        return_all_scores=True,\n",
    "        function_to_apply='sigmoid'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>release_date</th>\n",
       "      <th>likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>#0000FF</td>\n",
       "      <td>Siera</td>\n",
       "      <td>Welcome to my world\\n\\nRed , Blue , Green\\n\\nM...</td>\n",
       "      <td>2024.12.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>#RETURN(feat. 장건, Dysearth)</td>\n",
       "      <td>CYAN</td>\n",
       "      <td>how are you doing\\n있잖아 그건 나를 falling\\nI'm fall...</td>\n",
       "      <td>2024.10.16</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>#첫사랑</td>\n",
       "      <td>볼빨간사춘기</td>\n",
       "      <td>어릴 때 넌 키도 작고\\n빼빼 말랐지 기억나\\n난 knock 그런 니가\\n괜히 맘에...</td>\n",
       "      <td>2018.01.10</td>\n",
       "      <td>129991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>&amp; The Baby Girl (Feat. SABINE) (Original Mix)</td>\n",
       "      <td>George D.Blue</td>\n",
       "      <td>You and I and the baby girl\\nWhat should we be...</td>\n",
       "      <td>2024.12.15</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>'u' (Feat. Vann)</td>\n",
       "      <td>Wuno</td>\n",
       "      <td>baby 너만을 바라봐 나는 all day\\n알려줘 너만의 private한 stor...</td>\n",
       "      <td>2024.08.26</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4835</th>\n",
       "      <td>4835</td>\n",
       "      <td>흘러가자</td>\n",
       "      <td>유수이 (yusuii)</td>\n",
       "      <td>흘러가자 같이 내 시간은 버려두고\\n같은 곳을 보면서 share time\\n흘러가자...</td>\n",
       "      <td>2024.11.13</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4836</th>\n",
       "      <td>4836</td>\n",
       "      <td>희망의 빛</td>\n",
       "      <td>데이먼스 이어 (Damons year)</td>\n",
       "      <td>피로했던 발을 쉬게 할 수 없던 날\\n가망 없다는 걸 처음부터 알고 있었어\\n눈꺼풀...</td>\n",
       "      <td>2024.11.19</td>\n",
       "      <td>2541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4837</th>\n",
       "      <td>4837</td>\n",
       "      <td>힌트를 줘요 (Feat. 소라 (SORV))</td>\n",
       "      <td>aib (아입)</td>\n",
       "      <td>이렇게 긴 시간이 걸릴 이유가 대체 뭘까\\n혼자 있기엔 저 날씨가 아깝달까\\n\\n만...</td>\n",
       "      <td>2024.11.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4838</th>\n",
       "      <td>4838</td>\n",
       "      <td>힘든 거 알아</td>\n",
       "      <td>PATEKO (파테코)</td>\n",
       "      <td>아무 말이라도\\n네게 주고 싶은데\\n너에게 딱 알맞는 말이 없네\\n사람아\\n이젠 괜...</td>\n",
       "      <td>2022.12.29</td>\n",
       "      <td>11506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4839</th>\n",
       "      <td>4839</td>\n",
       "      <td>힙합하기로해써</td>\n",
       "      <td>처리 (Churry)</td>\n",
       "      <td>짝사랑은 또 처음이라서\\n실패할 사랑일 걸 알아서\\n이왕 망할 거 힘껏 하기로 했어...</td>\n",
       "      <td>2021.01.28</td>\n",
       "      <td>13570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4840 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                          title  \\\n",
       "0              0                                        #0000FF   \n",
       "1              1                    #RETURN(feat. 장건, Dysearth)   \n",
       "2              2                                           #첫사랑   \n",
       "3              3  & The Baby Girl (Feat. SABINE) (Original Mix)   \n",
       "4              4                               'u' (Feat. Vann)   \n",
       "...          ...                                            ...   \n",
       "4835        4835                                           흘러가자   \n",
       "4836        4836                                          희망의 빛   \n",
       "4837        4837                       힌트를 줘요 (Feat. 소라 (SORV))   \n",
       "4838        4838                                        힘든 거 알아   \n",
       "4839        4839                                        힙합하기로해써   \n",
       "\n",
       "                     artist  \\\n",
       "0                     Siera   \n",
       "1                      CYAN   \n",
       "2                    볼빨간사춘기   \n",
       "3             George D.Blue   \n",
       "4                      Wuno   \n",
       "...                     ...   \n",
       "4835           유수이 (yusuii)   \n",
       "4836  데이먼스 이어 (Damons year)   \n",
       "4837               aib (아입)   \n",
       "4838           PATEKO (파테코)   \n",
       "4839            처리 (Churry)   \n",
       "\n",
       "                                                 lyrics release_date   likes  \n",
       "0     Welcome to my world\\n\\nRed , Blue , Green\\n\\nM...   2024.12.31       0  \n",
       "1     how are you doing\\n있잖아 그건 나를 falling\\nI'm fall...   2024.10.16      23  \n",
       "2     어릴 때 넌 키도 작고\\n빼빼 말랐지 기억나\\n난 knock 그런 니가\\n괜히 맘에...   2018.01.10  129991  \n",
       "3     You and I and the baby girl\\nWhat should we be...   2024.12.15      50  \n",
       "4     baby 너만을 바라봐 나는 all day\\n알려줘 너만의 private한 stor...   2024.08.26      21  \n",
       "...                                                 ...          ...     ...  \n",
       "4835  흘러가자 같이 내 시간은 버려두고\\n같은 곳을 보면서 share time\\n흘러가자...   2024.11.13      10  \n",
       "4836  피로했던 발을 쉬게 할 수 없던 날\\n가망 없다는 걸 처음부터 알고 있었어\\n눈꺼풀...   2024.11.19    2541  \n",
       "4837  이렇게 긴 시간이 걸릴 이유가 대체 뭘까\\n혼자 있기엔 저 날씨가 아깝달까\\n\\n만...   2024.11.04       3  \n",
       "4838  아무 말이라도\\n네게 주고 싶은데\\n너에게 딱 알맞는 말이 없네\\n사람아\\n이젠 괜...   2022.12.29   11506  \n",
       "4839  짝사랑은 또 처음이라서\\n실패할 사랑일 걸 알아서\\n이왕 망할 거 힘껏 하기로 했어...   2021.01.28   13570  \n",
       "\n",
       "[4840 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"/home/wanted-1/potenup-workspace/Project/project2/team5/1.데이터모음/'music_data(Merge)'.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 텍스트 리스트를 잘라서 파이프라인에 전달\u001b[39;00m\n\u001b[1;32m     13\u001b[0m max_length \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mmodel_max_length  \u001b[38;5;66;03m# 일반적으로 512\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m truncated_texts \u001b[38;5;241m=\u001b[39m \u001b[43mtruncate_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# text 리스트의 각 텍스트를 최대 길이로 잘라냅니다.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m pip_text \u001b[38;5;241m=\u001b[39m pipe(truncated_texts)        \u001b[38;5;66;03m#잘라낸 텍스트를 파이프라인에 전달하여 감성 분석을 수행합니다.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 결과 저장을 위한 리스트\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mtruncate_texts\u001b[0;34m(texts, tokenizer, max_length)\u001b[0m\n\u001b[1;32m      6\u001b[0m truncated_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[0;32m----> 8\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m       \u001b[38;5;66;03m#텍스트 t를 토큰화할 때, 최대 길이 max_length로 잘라내고 필요하면 truncation=True로 설정하여 초과 부분을 제거합니다.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     truncated_texts\u001b[38;5;241m.\u001b[39mappend(tokenizer\u001b[38;5;241m.\u001b[39mdecode(tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))     \u001b[38;5;66;03m#잘라낸 토큰을 다시 텍스트로 변환하며, 특수 토큰은 제거합니다.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m truncated_texts\n",
      "File \u001b[0;32m~/miniconda3/envs/team5/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2868\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2866\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2867\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2868\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/team5/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2928\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2925\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2928\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2929\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2930\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2931\u001b[0m     )\n\u001b[1;32m   2933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2934\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2935\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2937\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "text = df['lyrics'].tolist()\n",
    "\n",
    "# 긴 텍스트 자르기 함수\n",
    "def truncate_texts(texts, tokenizer, max_length):   # 입력 텍스트 리스트를 받아, 각 텍스트를 최대 길이로 자르고 잘라낸 텍스트를 반환합니다.\n",
    "    truncated_texts = []\n",
    "    for t in texts:\n",
    "        tokens = tokenizer(t, max_length=max_length, truncation=True)       #텍스트 t를 토큰화할 때, 최대 길이 max_length로 잘라내고 필요하면 truncation=True로 설정하여 초과 부분을 제거합니다.\n",
    "        truncated_texts.append(tokenizer.decode(tokens['input_ids'], skip_special_tokens=True))     #잘라낸 토큰을 다시 텍스트로 변환하며, 특수 토큰은 제거합니다.\n",
    "    return truncated_texts\n",
    "\n",
    "# 텍스트 리스트를 잘라서 파이프라인에 전달\n",
    "max_length = tokenizer.model_max_length  # 일반적으로 512\n",
    "truncated_texts = truncate_texts(text, tokenizer, max_length)   # text 리스트의 각 텍스트를 최대 길이로 잘라냅니다.\n",
    "pip_text = pipe(truncated_texts)        #잘라낸 텍스트를 파이프라인에 전달하여 감성 분석을 수행합니다.\n",
    "\n",
    "\n",
    "# 결과 저장을 위한 리스트\n",
    "results = []\n",
    "\n",
    "# 각 감성에 해당할 확률(score)이 0.4 이상인 감성들만 저장\n",
    "for i, t in enumerate(truncated_texts):            # 잘라낸 텍스트와 그에 대한 감성 분석 결과를 반복합니다.\n",
    "    for output in pip_text[i]:\n",
    "        if output[\"score\"] > 0.4:\n",
    "            results.append({\n",
    "                \"text\": t,\n",
    "                \"label\": output[\"label\"],\n",
    "                \"score\": output[\"score\"]\n",
    "            })\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 데이터프레임 저장 \n",
    "results_df.to_csv(\"kote_result.csv\", index=False)\n",
    "\n",
    "# # 각 감성에 해당할 확률(score)이 0.4 이상인 감성들만 출력\n",
    "# for i, t in enumerate(truncated_texts):     # 잘라낸 텍스트와 그에 대한 감성 분석 결과를 반복합니다.\n",
    "#     print(f\"Text: {t}\")\n",
    "#     for output in pip_text[i]:\n",
    "#         if output[\"score\"] > 0.4:\n",
    "#             print(output)\n",
    "#     print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
